{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omnWjd7NRKFq"
   },
   "source": [
    "# Training Demo\n",
    "\n",
    "In this notebook we will run training script for the work [*Unsupervised Change Detection of Extreme Events Using ML On-Board*](http://arxiv.org/abs/2111.02995). This work was conducted at the [FDL Europe 2021](https://fdleurope.org/fdl-europe-2021) research accelerator program. \n",
    "\n",
    "**These instructions are meant to work on your local machine** (we don't use the Google Colab environment)\n",
    "\n",
    "*Note that in practice this takes long time, so this should serve only as an orientational demo.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o93G2PiIRKFt"
   },
   "source": [
    "## 1 Preparation\n",
    "\n",
    "- Get the dataset (for this demo we also provide a tiny training dataset subset - see below)\n",
    "\n",
    "- For better visualizations log into weights and biases with: wandb init\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofzitDKwRKFt"
   },
   "source": [
    "## 2 Libraries\n",
    "\n",
    "**Run these:**\n",
    "\n",
    "```\n",
    "make requirements\n",
    "conda activate ravaen_env\n",
    "conda install nb_conda\n",
    "jupyter notebook\n",
    "# start this notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GGNRwsr8SKVj"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p3RyYqyzRKFu",
    "outputId": "d9feb9bc-f0a6-4b1e-c5ca-7b5ba73538ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     active environment : ravaen_env\r\n"
     ]
    }
   ],
   "source": [
    "!conda info | grep 'active environment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gg1167yWRKFx",
    "outputId": "32651f09-03ac-4c6e-dba0-e932a6c91262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan  7 16:22:08 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.05              Driver Version: 545.84       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    On  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   49C    P0              12W /  42W |      0MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OS17PWKWSMpV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1rl3Clf0c7HlXnlPXO837Pjr2iCjwak0Y\n",
      "From (redirected): https://drive.google.com/uc?id=1rl3Clf0c7HlXnlPXO837Pjr2iCjwak0Y&confirm=t&uuid=d37337b2-4d62-4e13-9a2d-8a23a97a6fbe\n",
      "To: /home/lucap/l46/l46-project/RaVAEn-master/notebooks/train_minisubset.zip\n",
      "100%|████████████████████████████████████████| 658M/658M [00:31<00:00, 20.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# The official training dataset is much larger, for the purpose of the demo, we provide a small subset:\n",
    "!gdown https://drive.google.com/uc?id=1rl3Clf0c7HlXnlPXO837Pjr2iCjwak0Y -O train_minisubset.zip\n",
    "!unzip -q train_minisubset.zip\n",
    "!rm train_minisubset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwc457wzRKFx"
   },
   "source": [
    "**Edit the paths in config/config.yaml**\n",
    "\n",
    "```\n",
    "log_dir: \"/home/<USER>/results\"\n",
    "cache_dir: \"/home/<USER>/cache\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "633WMEtkRKFy",
    "outputId": "f375b444-7d01-44a5-eb18-1df41db51107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\r\n",
      "entity: \"mlpayloads\"\r\n",
      "\r\n",
      "# FL setup\r\n",
      "fraction_fit: 0.0001\r\n",
      "fraction_eval: 0.001\r\n",
      "min_fit_clients: 20\r\n",
      "min_eval_clients: 20\r\n",
      "local_epochs: 10\r\n",
      "num_rounds: 5\r\n",
      "num_clients: 20\r\n",
      "config_fit:\r\n",
      "  lr: 0.001\r\n",
      "  momentum: 0.9\r\n",
      "  local_epochs: 10\r\n",
      "\r\n",
      "# RaVAEn setup\r\n",
      "log_dir: \"/home/josie/l46/l46-project/RaVAEn-master/outputs/results\"\r\n",
      "cache_dir: \"/home/josie/l46/l46-project/RaVAEn-master/outputs/cache\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../config/config.yaml\n",
    "\"\"\"\n",
    "Fill in:\n",
    "log_dir: \"/home/<USER>/results\"\n",
    "cache_dir: \"/home/<USER>/cache\"\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/josie/l46/l46-project/RaVAEn-master/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/josie/l46/l46-project/RaVAEn-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/josie/l46/l46-project/RaVAEn-master\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "EaNE7JUlRKFy",
    "outputId": "d714853b-442f-4a60-b856-376f011c4cf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/josie/anaconda3/envs/ravaen_env/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/josie/l46/l46-project/RaVAEn-master/scripts/train_model.py:14: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path='../config', config_name='config.yaml')\n",
      "/home/josie/anaconda3/envs/ravaen_env/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "Global seed set to 42\n",
      "Error executing job with overrides: ['+dataset=alpha_multiscene_tiny', '++dataset.root_folder=/home/lucap/l46/l46-project/RaVAEn-master/notebooks/train_minisubset', '+normalisation=log_scale', '+channels=high_res', '+training=simple_vae', '+module=deeper_vae', '+project=train_VAE_128small', '+name=VAE_128small', 'module.model_cls_args.latent_dim=128', 'module.model_cls_args.extra_depth_on_scale=0', 'module.model_cls_args.hidden_channels=[16,32,64]', 'training.epochs=100']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/josie/l46/l46-project/RaVAEn-master/scripts/train_model.py\", line 21, in main\n",
      "    data_module = ParsedDataModule.load_or_create(cfg['dataset'],\n",
      "  File \"/home/josie/l46/l46-project/RaVAEn-master/src/data/datamodule.py\", line 181, in load_or_create\n",
      "    datamodule = ParsedDataModule(cfg)\n",
      "  File \"/home/josie/l46/l46-project/RaVAEn-master/src/data/datamodule.py\", line 30, in __init__\n",
      "    self.train_ds = self.create_dataset(self.root_folder, self.train,\n",
      "  File \"/home/josie/l46/l46-project/RaVAEn-master/src/data/datamodule.py\", line 60, in create_dataset\n",
      "    return MyConcatDataset(datasets)\n",
      "  File \"/home/josie/l46/l46-project/RaVAEn-master/src/data/datamodule.py\", line 220, in __init__\n",
      "    super().__init__(datasets)\n",
      "  File \"/home/josie/anaconda3/envs/ravaen_env/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 238, in __init__\n",
      "    assert len(datasets) > 0, 'datasets should not be an empty iterable'  # type: ignore[arg-type]\n",
      "AssertionError: datasets should not be an empty iterable\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    }
   ],
   "source": [
    "# ===== Parameters to adjust =====\n",
    "epochs = 100\n",
    "dataset_root_folder = \"/home/lucap/l46/l46-project/RaVAEn-master/notebooks/train_minisubset\"\n",
    "dataset=\"alpha_multiscene_tiny\" # for the demo, for the full training dataset we would use: dataset=\"alpha_multiscene\"\n",
    "\n",
    "name=\"VAE_128small\" # note \"small\" uses these settings > module.model_cls_args.latent_dim=128 module.model_cls_args.extra_depth_on_scale=0 module.model_cls_args.hidden_channels=[16,32,64]\n",
    "\n",
    "# ===== Parameters to keep the same ======\n",
    "training=\"simple_vae\"\n",
    "module=\"deeper_vae\"\n",
    "\n",
    "# ========================================\n",
    "\n",
    "#python3 -m scripts.train_model +dataset=alpha_multiscene_tiny ++dataset.root_folder=\"/home/lucap/l46/l46-project/RaVAEn-master/notebooks/train_minisubset\" +normalisation=log_scale +channels=high_res +training=simple_vae +module=deeper_vae +project=train_VAE_128small +name=\"VAE_128small\" module.model_cls_args.latent_dim=128 module.model_cls_args.extra_depth_on_scale=0 module.model_cls_args.hidden_channels=[16,32,64] training.epochs=100\n",
    "\n",
    "!python3 -m scripts.train_model +dataset=$dataset ++dataset.root_folder=\"{dataset_root_folder}\" \\\n",
    "         +normalisation=log_scale +channels=high_res +training=$training +module=$module +project=train_VAE_128small +name=\"{name}\" \\\n",
    "         module.model_cls_args.latent_dim=128 module.model_cls_args.extra_depth_on_scale=0 module.model_cls_args.hidden_channels=[16,32,64] \\\n",
    "         training.epochs=$epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/josie/anaconda3/envs/ravaen_env/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Training on cpu using PyTorch 1.9.0 and Flower 1.6.0\n",
      "/home/josie/l46/l46-project/RaVAEn-master/scripts/main.py:19: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"../config\", config_name=\"config\")\n",
      "/home/josie/anaconda3/envs/ravaen_env/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "Global seed set to 42\n",
      "\n",
      "Preprocessing dataset...\n",
      "Extracted latent_dim=128 from config.yaml\n",
      "Extracted vis_channels=[2, 1, 0] from config.yaml\n",
      "\n",
      "Datamodule created!\n",
      "Partitioned 12180 of 12190 training set entries into 20 partitions of length 609\n",
      "Training set length: 12180\n",
      "Sum of partitions: 12180\n",
      "\n",
      "FedRaVAEn dataset loaded!\n",
      "Number of training loaders: 20, Number of Validation Loaders: 20\n",
      "Length of each partition's dataset: 549 (training), 60 (validation)\n",
      "Length of test dataset: 726\n",
      "Initialising Ray runtime..\n",
      "2024-01-08 17:31:56,610\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "2024-01-08 17:31:56,707\tINFO packaging.py:518 -- Creating a file package for local directory '/home/josie/l46/l46-project/RaVAEn-master/'.\n",
      "2024-01-08 17:31:56,768\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2991b8b7354abba7.zip' (5.81MiB) to Ray cluster...\n",
      "2024-01-08 17:31:56,786\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2991b8b7354abba7.zip'.\n",
      "Success!\n",
      "\n",
      "Generating clients...\n",
      "Success!\n",
      "Creating strategy...\n",
      "Success!\n",
      "Starting Simulation...\n",
      "INFO flwr 2024-01-08 17:31:58,438 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
      "[2024-01-08 17:31:58,438][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
      "2024-01-08 17:32:01,890\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "2024-01-08 17:32:01,901\tINFO packaging.py:518 -- Creating a file package for local directory '/home/josie/l46/l46-project/RaVAEn-master/src'.\n",
      "2024-01-08 17:32:01,912\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_128d60c6f2187559.zip' (0.28MiB) to Ray cluster...\n",
      "2024-01-08 17:32:01,913\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_128d60c6f2187559.zip'.\n",
      "INFO flwr 2024-01-08 17:32:02,669 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'node:__internal_head__': 1.0, 'node:172.30.248.78': 1.0, 'memory': 4068107060.0, 'object_store_memory': 2034053529.0}\n",
      "[2024-01-08 17:32:02,669][flwr][INFO] - Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'node:__internal_head__': 1.0, 'node:172.30.248.78': 1.0, 'memory': 4068107060.0, 'object_store_memory': 2034053529.0}\n",
      "INFO flwr 2024-01-08 17:32:02,670 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "[2024-01-08 17:32:02,670][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "INFO flwr 2024-01-08 17:32:02,670 | app.py:227 | No `client_resources` specified. Using minimal resources for clients.\n",
      "[2024-01-08 17:32:02,670][flwr][INFO] - No `client_resources` specified. Using minimal resources for clients.\n",
      "INFO flwr 2024-01-08 17:32:02,670 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "[2024-01-08 17:32:02,670][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "INFO flwr 2024-01-08 17:32:02,678 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors\n",
      "[2024-01-08 17:32:02,678][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 8 actors\n",
      "INFO flwr 2024-01-08 17:32:02,678 | server.py:89 | Initializing global parameters\n",
      "[2024-01-08 17:32:02,678][flwr][INFO] - Initializing global parameters\n",
      "INFO flwr 2024-01-08 17:32:02,678 | server.py:272 | Using initial parameters provided by strategy\n",
      "[2024-01-08 17:32:02,678][flwr][INFO] - Using initial parameters provided by strategy\n",
      "INFO flwr 2024-01-08 17:32:02,678 | server.py:91 | Evaluating initial parameters\n",
      "[2024-01-08 17:32:02,678][flwr][INFO] - Evaluating initial parameters\n",
      "INFO flwr 2024-01-08 17:32:02,678 | server.py:104 | FL starting\n",
      "[2024-01-08 17:32:02,678][flwr][INFO] - FL starting\n",
      "DEBUG flwr 2024-01-08 17:32:02,679 | server.py:222 | fit_round 1: strategy sampled 6 clients (out of 20)\n",
      "[2024-01-08 17:32:02,679][flwr][DEBUG] - fit_round 1: strategy sampled 6 clients (out of 20)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74058)\u001b[0m /home/josie/anaconda3/envs/ravaen_env/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.1)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74058)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74058)\u001b[0m Training on cpu using PyTorch 1.9.0 and Flower 1.6.0\n",
      "  0%|          | 0/3 [00:00<?, ?it/s] \n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74055)\u001b[0m [Client 12] fit, config: {}\n",
      " 33%|███▎      | 1/3 [00:17<00:35, 17.99s/it]\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74056)\u001b[0m /home/josie/anaconda3/envs/ravaen_env/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.1)\u001b[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74056)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      " 67%|██████▋   | 2/3 [00:23<00:10, 10.46s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "100%|██████████| 3/3 [00:23<00:00,  8.00s/it]\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74058)\u001b[0m Epoch 1: train loss nan\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74056)\u001b[0m Training on cpu using PyTorch 1.9.0 and Flower 1.6.0\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74056)\u001b[0m [Client 10] fit, config: {}\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "DEBUG flwr 2024-01-08 17:32:29,882 | server.py:236 | fit_round 1 received 6 results and 0 failures\n",
      "[2024-01-08 17:32:29,882][flwr][DEBUG] - fit_round 1 received 6 results and 0 failures\n",
      "WARNING flwr 2024-01-08 17:32:29,975 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "[2024-01-08 17:32:29,975][flwr][WARNING] - No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2024-01-08 17:32:29,975 | server.py:173 | evaluate_round 1: strategy sampled 6 clients (out of 20)\n",
      "[2024-01-08 17:32:29,975][flwr][DEBUG] - evaluate_round 1: strategy sampled 6 clients (out of 20)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74056)\u001b[0m [Client 2] evaluate, config: {}\n",
      "DEBUG flwr 2024-01-08 17:32:32,032 | server.py:187 | evaluate_round 1 received 6 results and 0 failures\n",
      "[2024-01-08 17:32:32,032][flwr][DEBUG] - evaluate_round 1 received 6 results and 0 failures\n",
      "WARNING flwr 2024-01-08 17:32:32,032 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
      "[2024-01-08 17:32:32,032][flwr][WARNING] - No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2024-01-08 17:32:32,032 | server.py:222 | fit_round 2: strategy sampled 6 clients (out of 20)\n",
      "[2024-01-08 17:32:32,032][flwr][DEBUG] - fit_round 2: strategy sampled 6 clients (out of 20)\n",
      "  0%|          | 0/3 [00:00<?, ?it/s] \n",
      " 33%|███▎      | 1/3 [00:16<00:33, 16.65s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "100%|██████████| 3/3 [00:23<00:00,  8.00s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      " 67%|██████▋   | 2/3 [00:22<00:09,  9.96s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=74060)\u001b[0m Epoch 1: train loss nan\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74055)\u001b[0m [Client 11] fit, config: {}\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74057)\u001b[0m [Client 15] evaluate, config: {}\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "100%|██████████| 3/3 [00:22<00:00,  7.49s/it]\n",
      "DEBUG flwr 2024-01-08 17:32:55,466 | server.py:236 | fit_round 2 received 6 results and 0 failures\n",
      "[2024-01-08 17:32:55,466][flwr][DEBUG] - fit_round 2 received 6 results and 0 failures\n",
      "DEBUG flwr 2024-01-08 17:32:55,540 | server.py:173 | evaluate_round 2: strategy sampled 6 clients (out of 20)\n",
      "[2024-01-08 17:32:55,540][flwr][DEBUG] - evaluate_round 2: strategy sampled 6 clients (out of 20)\n",
      "DEBUG flwr 2024-01-08 17:32:57,720 | server.py:187 | evaluate_round 2 received 6 results and 0 failures\n",
      "[2024-01-08 17:32:57,720][flwr][DEBUG] - evaluate_round 2 received 6 results and 0 failures\n",
      "DEBUG flwr 2024-01-08 17:32:57,720 | server.py:222 | fit_round 3: strategy sampled 6 clients (out of 20)\n",
      "[2024-01-08 17:32:57,720][flwr][DEBUG] - fit_round 3: strategy sampled 6 clients (out of 20)\n",
      "  0%|          | 0/3 [00:00<?, ?it/s] \n",
      " 33%|███▎      | 1/3 [00:16<00:33, 16.69s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "100%|██████████| 3/3 [00:23<00:00,  7.74s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      " 67%|██████▋   | 2/3 [00:22<00:09,  9.87s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "100%|██████████| 3/3 [00:22<00:00,  7.49s/it]\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74058)\u001b[0m Epoch 1: train loss nan\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74056)\u001b[0m [Client 12] fit, config: {}\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=74058)\u001b[0m [Client 13] evaluate, config: {}\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "DEBUG flwr 2024-01-08 17:33:21,525 | server.py:236 | fit_round 3 received 6 results and 0 failures\n",
      "[2024-01-08 17:33:21,525][flwr][DEBUG] - fit_round 3 received 6 results and 0 failures\n",
      "DEBUG flwr 2024-01-08 17:33:21,604 | server.py:173 | evaluate_round 3: strategy sampled 6 clients (out of 20)\n",
      "[2024-01-08 17:33:21,604][flwr][DEBUG] - evaluate_round 3: strategy sampled 6 clients (out of 20)\n"
     ]
    }
   ],
   "source": [
    "## MAIN\n",
    "# ===== Parameters to adjust =====\n",
    "epochs = 100\n",
    "dataset_root_folder = \"/home/josie/l46/l46-project/phoenix/train_minisubset\"\n",
    "dataset=\"alpha_multiscene_tiny\" # for the demo, for the full training dataset we would use: dataset=\"alpha_multiscene\"\n",
    "\n",
    "# note \"small\" uses these setting:\n",
    "# module.model_cls_args.latent_dim=128 \n",
    "# module.model_cls_args.extra_depth_on_scale=0 \n",
    "# module.model_cls_args.hidden_channels=[16,32,64]\n",
    "name=\"VAE_128small\" \n",
    "\n",
    "# ===== Parameters to keep the same ======\n",
    "training=\"simple_vae\"\n",
    "module=\"deeper_vae\"\n",
    "\n",
    "# ========================================\n",
    "\n",
    "!HYDRA_FULL_ERROR=1 python -m scripts.main +dataset=$dataset ++dataset.root_folder=\"{dataset_root_folder}\" +training=$training \\\n",
    "        +module=$module +normalisation=log_scale +channels=high_res +name=\"{name}\" module.model_cls_args.latent_dim=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/josie/l46/l46-project/RaVAEn-master\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing job with overrides: ['+dataset=alpha_multiscene_tiny', '++dataset.root_folder=/home/lucap/l46/l46-project/RaVAEn-master/notebooks/train_minisubset']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lucap/l46/l46-project/RaVAEn-master/scripts/make_datamodule.py\", line 14, in main\n",
      "    cfg = deepconvert(cfg)\n",
      "  File \"/home/lucap/l46/l46-project/RaVAEn-master/src/utils.py\", line 26, in deepconvert\n",
      "    not_omega_conf.update({k: deepconvert(v)})\n",
      "  File \"/home/lucap/l46/l46-project/RaVAEn-master/src/utils.py\", line 26, in deepconvert\n",
      "    not_omega_conf.update({k: deepconvert(v)})\n",
      "  File \"/home/lucap/l46/l46-project/RaVAEn-master/src/utils.py\", line 25, in deepconvert\n",
      "    for k, v in omega_conf.items():\n",
      "omegaconf.errors.InterpolationKeyError: Interpolation key 'training.batch_size_train' not found\n",
      "    full_key: dataset.train.batch_size\n",
      "    object_type=dict\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    }
   ],
   "source": [
    "dataset_root_folder = \"/home/lucap/l46/l46-project/RaVAEn-master/notebooks/train_minisubset\"\n",
    "dataset=\"alpha_multiscene_tiny\" # for the demo, for the full training dataset we would use: dataset=\"alpha_multiscene\"\n",
    "\n",
    "!python3 -m scripts.make_datamodule +dataset=$dataset ++dataset.root_folder=\"{dataset_root_folder}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbYzpJwXRKFz"
   },
   "source": [
    "### More advanced settings:\n",
    "\n",
    "See the possible options using --help and then looking at the individual configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgxa-F3zRKF0",
    "outputId": "e7314858-d5d3-4c2d-9ab5-dd76538031e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model is powered by Hydra.\r\n",
      "\r\n",
      "== Configuration groups ==\r\n",
      "Compose your configuration from those groups (group=option)\r\n",
      "\r\n",
      "channels: all, high_res, high_res_phisat2overlap, rgb, rgb_nir, rgb_nir_b11, rgb_nir_b11_b12_landsat, rgb_nir_b12\r\n",
      "dataset: alpha_multiscene, alpha_multiscene_tiny, alpha_singlescene, dataloader_test, eval, fire, fires, floods_evaluation, hurricanes, landslides, landslides_2, oilspills, preliminary, preliminary_da, preliminary_multiscene, preliminary_sequential, preliminary_sequential_bigger, preliminary_sequential_bigger_9k, preliminary_sequential_bigger_multiEval, preliminary_sequential_bigger_multiEval_Germany, samples_for_gui, temporal_analysis, volcanos\r\n",
      "evaluation: ae_base, ae_fewer, vae_base, vae_da, vae_da_8px, vae_fewer, vae_paper\r\n",
      "module: deeper_ae, deeper_ae_bigger_latent, deeper_vae, grx, simple_ae, simple_ae_with_linear, simple_vae\r\n",
      "normalisation: log_scale, none\r\n",
      "training: da, simple_ae, simple_vae\r\n",
      "transform: eval_da, eval_da_8px, eval_nda, eval_nda_8px, none, random, random_1px, random_4px, random_6px, simple\r\n",
      "\r\n",
      "\r\n",
      "== Config ==\r\n",
      "Override anything in the config (foo.bar=value)\r\n",
      "\r\n",
      "entity: mlpayloads\r\n",
      "log_dir: /home/vitek/fdl_tmp/results\r\n",
      "cache_dir: /home/vitek/fdl_tmp/cache\r\n",
      "\r\n",
      "\r\n",
      "Powered by Hydra (https://hydra.cc)\r\n",
      "Use --hydra-help to view Hydra specific help\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m scripts.train_model --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_cDsLxHRKF1",
    "outputId": "21b4ec6b-3ca7-4382-cf57-c2f3adc98ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\r\n",
      "gpus: -1\r\n",
      "epochs: 400\r\n",
      "grad_batches: 1\r\n",
      "distr_backend: 'dp'\r\n",
      "use_amp: true # ... true = 16 precision / false = 32 precision\r\n",
      "\r\n",
      "# The check_val_every_n_epoch and val_check_interval settings overlap, see:\r\n",
      "#     https://github.com/PyTorchLightning/pytorch-lightning/issues/6385\r\n",
      "val_check_interval: 0.2  # either in to check after that many batches or float to check that fraction of epoch\r\n",
      "check_val_every_n_epoch: 1 \r\n",
      "\r\n",
      "fast_dev_run: false\r\n",
      "\r\n",
      "num_workers: 16\r\n",
      "\r\n",
      "batch_size_train: 256\r\n",
      "batch_size_valid: 256\r\n",
      "batch_size_test: 256\r\n",
      "\r\n",
      "lr: 0.001\r\n",
      "weight_decay: 0.0\r\n",
      "# scheduler_gamma: 0.95\r\n",
      "\r\n",
      "# auto_batch_size: 'binsearch'\r\n",
      "#auto_lr: 'lr'\r\n"
     ]
    }
   ],
   "source": [
    "# to see the detiled options for \"training: da, simple_ae, simple_vae\"\n",
    "!cat config/training/simple_vae.yaml\n",
    "# for example we would then set epochs with adding this to the main command:\n",
    "# training.epochs=1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "training_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
